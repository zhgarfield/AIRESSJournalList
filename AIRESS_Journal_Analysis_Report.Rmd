---
title: "AIRESS Behavioral Science Journal Analysis"
author: "Pr. Zachary Garfield"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
---

```{r setup, include=FALSE, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
source("processing.R")
library(DT)
```

# Introduction

The goal of this project is to build a transparent, evidence-based classification of academic journals relevant to the interdisciplinary Behavioral Sciences at AIRESS and UM6P. Faculty and students routinely make decisions about where to publish, which journals to prioritize, and how to interpret publication records. Yet journal quality is uneven across fields, citation practices differ substantially by discipline, and intuitive judgments are often shaped by disciplinary silos. This report therefore develops a principled, reproducible system for evaluating journals within their appropriate disciplinary contexts, while remaining comparable across the broader behavioral sciences.

We begin by assembling and cleaning a curated set of Scopus journal lists across subject areas aligned with behavioral science, including psychology, anthropology, decision sciences, demography, sociology, philosophy of science, and applied social sciences. These data are harmonized into a single dataset, with journals allowed to appear in multiple subject areas to reflect their interdisciplinary positioning.

Rather than imposing a single, global ranking across all fields, we adopt a within-subcategory evaluation strategy. Journals are grouped into a small number of disciplinary subcategories, and all subsequent analyses—impact estimation, filtering, and tiering—are conducted within each subcategory. This approach respects differences in citation density, publication norms, and scale across disciplines, while still producing a coherent and transparent evaluation framework.                         |

To assess journal impact in a unified and data-driven way, we conduct a Principal Component Analysis (PCA) on three widely used bibliometric indicators—CiteScore, SNIP, and SJR—within each subcategory. PCA extracts the shared underlying signal among these metrics, producing a single latent “impact” dimension (PC1) that summarizes overall journal standing without overweighting any single citation measure. This approach yields an interpretable measure of impact that is calibrated to disciplinary context rather than distorted by cross-field differences in citation practices.

To improve interpretability and avoid distortions from extremely low-impact or marginal journals, we remove the bottom 25% of titles within each subcategory based on their PC1 scores. These journals contribute little to meaningful differentiation among commonly targeted outlets and would otherwise compress the scale, making it harder to distinguish differences among mid- and high-tier journals. Filtering at this stage ensures that the resulting impact index and tiering system reflect journals most relevant to the research and teaching missions of AIRESS.

Finally, we convert the latent impact scores into an intuitive 0–100 Impact Index and implement a tiering procedure using k-means clustering within each subcategory. Journals are classified into three categories—A (Excellent), B (Preferred), and C (Acceptable)—based on natural groupings in the data. The result is a clear, defensible framework for evaluating journals across the behavioral sciences while preserving sensitivity to disciplinary context.

# Data Sources

Scopus is the most comprehensive and widely used bibliometric database in the behavioral and social sciences. To capture the breadth of scholarship represented at AIRESS, we assembled a curated set of Scopus subject categories most relevant to interdisciplinary behavioral science. In total, `r length(unique(data$subject_area))` Scopus subject areas were included, spanning psychology, anthropology, decision sciences, demography, sociology, philosophy of science, applied social sciences, and related domains.

Because Scopus subject categories vary widely in scope and often overlap, individual subject areas were grouped into a smaller number of analytically meaningful AIRESS subcategories. This step serves two purposes. First, it reflects how behavioral science is organized in practice at AIRESS, where research and training span multiple adjacent Scopus classifications. Second, it provides an appropriate unit for within-discipline evaluation, ensuring that journals are compared against relevant peers rather than across fields with fundamentally different citation norms.

The mapping between Scopus subject categories and AIRESS subcategories used in this analysis is shown in Table 1.

Table 1. Mapping between Scopus subject categories and AIRESS subcategories used in this analysis.

| **AIRESS Subcategory**      | **Scopus Subject Categories Included**                                                                                                                                 |
| --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Decision Sciences**       | Decision Sciences (Miscellaneous); General Decision Sciences                                                                                                           |
| **Psychology**              | Psychology (Miscellaneous); General Psychology; Applied Psychology; Experimental and Cognitive Psychology; Social Psychology; Developmental and Educational Psychology |
| **Anthropology**            | Anthropology; Cultural Studies                                                                                                                                         |
| **Demography**              | Demography                                                                                                                                                             |
| **Sociology**               | Sociology and Political Science                                                                                                                                        |
| **Philosophy of Science**   | History and Philosophy of Science                                                                                                                                      |
| **Applied Social Sciences** | Health (Social Science)                                                                                                                                                |
| **Multidisciplinary**       | Multidisciplinary                                                                                                                                                      |
| **Other**                   | Social Sciences (Miscellaneous); General Social Sciences; General Agriculture and Biological Sciences; General Medicine                                                |

For each Scopus subject category, a Source List (Excel format) was downloaded containing all indexed journals along with their bibliometric indicators, including CiteScore, SNIP (Source Normalized Impact per Paper), SJR (SCImago Journal Rank), publisher information, and associated metadata. Only journal sources were included, and only those ranked in the first or second Scopus quartile (Q1–Q2), in order to focus the analysis on established and widely recognized outlets.

Two journals—*MMWR Surveillance Summaries* and *MMWR Recommendations and Reports*—were removed at the outset. Although highly cited, these outlets function primarily as epidemiological policy and surveillance reports and fall outside the scope of behavioral science publishing. Their inclusion would disproportionately influence the citation structure and distort subsequent analyses.

All bibliometric variables were then coerced to numeric format, and journals with missing values for any of the three core metrics (CiteScore, SNIP, or SJR) were excluded. This ensures that all subsequent PCA and clustering analyses are based on complete and directly comparable citation information.

All data correspond to the 2024 Scopus Source Lists. Only subject categories judged to be substantively relevant to behavioral science research and training at AIRESS were included in the analysis.

# Data Import and Preparation

Each Scopus subject file was imported, assigned a subject-area label extracted from the filename, and merged into a single dataset. Because many journals are indexed under multiple Scopus subject categories, duplicate journal titles were consolidated while preserving all subject-area memberships through a binary indicator matrix. This ensures that interdisciplinary journals retain their full classification profiles rather than being forced into a single category.

After cleaning and consolidation, the dataset contains `r dim(journal_metadata)[1]` unique journals across all selected Scopus subject areas. Journals may later appear multiple times in the analysis when evaluated within different AIRESS subcategories, reflecting their interdisciplinary scope.

The three core bibliometric indicators—CiteScore, SNIP, and SJR—were then coerced to numeric format. Journals with missing values for any of these metrics were excluded to ensure a complete and consistent basis for subsequent PCA, impact index construction, and clustering analyses.

# PCA and Construction of the Impact Index

Our objective was to construct a unified, transparent measure of journal impact that remains sensitive to disciplinary context. The three citation metrics provided by Scopus—CiteScore, SNIP, and SJR—are correlated, measured on different scales, and shaped by field-specific citation practices. Rather than treating them separately or privileging a single metric, we use Principal Component Analysis (PCA) to extract their shared underlying signal.

PCA is conducted within each AIRESS subcategory, using CiteScore, SNIP, and SJR as inputs. Within each subcategory, the first principal component (PC1) captures the dominant pattern of variation across the three metrics and serves as a latent impact dimension.

Figure \@ref(fig:pca-loadings) shows the PC1 loadings for each subcategory. Across all subcategories, CiteScore, SNIP, and SJR load strongly and in the same direction on PC1, indicating that they consistently reflect a common underlying construct: overall journal influence and visibility within the discipline. While the exact magnitude of the loadings varies slightly across subcategories, the structure is highly stable, confirming that PC1 provides a coherent and interpretable summary of journal impact in each field.

Higher-order components (PC2 and PC3) account for only small residual variation and do not correspond to interpretable dimensions of journal quality. For this reason, only PC1 is retained for the construction of the Impact Index and for all subsequent analyses.

```{r pca-loadings, fig.cap="PC1 loadings by subcategory. CiteScore, SNIP, and SJR load strongly and in the same direction across all subcategories, indicating a stable latent dimension of journal impact. Higher-order components capture only minor residual variation and are not used in subsequent analyses.", fig.width=8}
pc1_loadings_by_subcat_plot
```

This subcategory-specific PCA strategy allows us to summarize journal impact in a way that is both data-driven and discipline-aware, forming the foundation for the subsequent construction of the Impact Index and tiering system.

Across all subcategories, the three citation metrics load strongly and in the same direction on PC1, indicating that they reflect a common latent construct: overall journal influence, visibility, and citation performance within disciplinary context. Because PCA axes have no intrinsic orientation, PC1 was aligned so that higher values correspond to higher CiteScore, ensuring consistent interpretability across subcategories. We then examined the distribution of PC1 scores within each subcategory. As expected, these distributions are strongly right-skewed: most journals have modest citation profiles, while a smaller number occupy a high-impact tail.

To avoid distorting subsequent clustering and tiering by including very low-impact journals that form a distinct bottom group, we removed journals in the lowest quartile of the PC1 distribution within each subcategory. These outlets contribute little information for distinguishing meaningful tiers and tend to cluster separately from journals that faculty and students are most likely to target. Applying this filter yields a reduced dataset of journals that more accurately reflects the active publishing landscape relevant to AIRESS.

# Scaling PC1 and Construction of the Impact Index

After filtering, PC1 scores were rescaled within each subcategory to a 0–100 Impact Index. A value of 0 represents the lowest-impact journals retained in that subcategory, while a value of 100 represents the highest-impact journals. Intermediate values reflect proportional standing along the latent impact dimension.

The distribution of the Impact Index, shown in Figure \@ref(fig:index-hist), is strongly right-skewed (mean = `r round(mean(df_final$impact_index), 2)`, SD = `r round(sd(df_final$impact_index), 2)`). Most journals cluster toward the lower end of the index, while a small number extend into the high-impact range. This asymmetry reflects a well-established feature of citation-based metrics, in which scholarly influence is concentrated among a limited set of journals rather than evenly distributed across outlets.

Rescaling PC1 in this way produces an intuitive and interpretable metric that preserves relative differences in impact while placing journals on a standardized scale suitable for clustering and tier assignment.

```{r index-hist, fig.cap="The Impact Index (0–100) is derived from the first principal component (PC1) of CiteScore, SNIP, and SJR. Y-axes are scaled independently within each subcategory to emphasize the internal structure of impact distributions rather than differences in the number of journals across fields. The distribution is highly right-skewed, with most journals concentrated at the lower end of the impact spectrum and a long tail of increasingly influential outlets. This pattern reflects the typical unevenness of citation-based metrics, where a small subset of journals capture disproportionately high scholarly visibility and influence."}
index_hist_by_subcat
```

# K-Means Clustering and Construction of the Tier System

After filtering out the lowest-impact quartile of journals and rescaling PC1 to a standardized 0–100 Impact Index, the next step was to translate this continuous measure into a practical, categorical tiering system usable by faculty, students, and administrators. Rather than imposing arbitrary numeric thresholds, our goal was to identify natural groupings in journal impact within each disciplinary context. For this reason, we applied k-means clustering, a widely used unsupervised learning method that partitions observations into groups based on similarity.

K-means clustering was applied separately within each AIRESS subcategory, using the Impact Index as the input variable and specifying three clusters. These clusters correspond to the three tiers used in the evaluation framework: A (Excellent), B (Preferred), and C (Acceptable). Conducting clustering within subcategories ensures that journals are grouped relative to appropriate disciplinary peers, rather than being compared across fields with very different citation norms and publication scales.

K-means assigns each journal to a cluster characterized by a centroid representing the mean Impact Index of that group. Because cluster labels are inherently unordered, clusters were subsequently sorted by their mean Impact Index and mapped onto qualitative tiers: the highest-mean cluster was labeled Tier A (Excellent), the intermediate cluster Tier B (Preferred), and the lowest-mean cluster Tier C (Acceptable). This post hoc ordering guarantees that tier labels always reflect relative impact, regardless of the numeric cluster identifiers produced by the algorithm. See \@ref(fig:clusterplot).

```{r clusterplot, fig.cap="K-means cluster centers by subcategory. The figure shows the mean Impact Index for each cluster within disciplinary subcategories. Clusters are ordered post hoc by mean impact, and tier labels (Excellent, Preferred, Acceptable) reflect relative standing rather than the numeric cluster identifiers produced by k-means."}
cluster_centers_plot_all
```

This procedure yields a clear, data-driven tier structure within each subcategory:

* Tier A (Excellent): The highest-impact and most influential journals within a given disciplinary context. These outlets typically exhibit strong citation performance, broad visibility, and are widely recognized as leading journals in their fields.

* Tier B (Preferred): Journals with solid and reliable impact profiles. These outlets are well regarded within their disciplines and represent competitive publication venues for faculty and graduate students.

* Tier C (Acceptable): Journals with modest but legitimate impact. These may be appropriate venues for specialized topics, methodological contributions, or early-stage researchers, while still meeting basic standards of scholarly quality.

To evaluate how clearly the k-means clustering separates journals within each subcategory, we examine the empirical cumulative distribution functions (ECDFs) of the Impact Index by cluster, shown in Figure \@ref(fig:ecdf_plot_all). Unlike summary statistics or cluster centers alone, ECDFs display the full distribution of journal impact within each tier, allowing us to assess whether higher-tier clusters consistently dominate lower-tier clusters across the entire impact range. Curves that lie further to the right indicate clusters with systematically higher impact, while the degree of overlap between curves reflects how sharply tiers are differentiated within a given subcategory. Where ECDF curves are well separated, tier boundaries are clear; where they overlap or cross, distinctions between tiers are more gradual, reflecting continuous variation or heterogeneity in citation practices within the field. This visualization therefore provides an important diagnostic check on the robustness and interpretability of the tiering system across disciplinary contexts.

```{r ecdfplot, fig.cap="ECDFs of the Impact Index by cluster and subcategory. Right-shifted curves correspond to higher-impact tiers; overlap indicates continuous variation. Cluster IDs are arbitrary and mapped to tiers post hoc by mean impact."}
ecdf_plot_all
```

As shown in Figure \@ref(fig:tierplot), the resulting tier distributions exhibit clear stratification of journal impact within subcategories. Most journals fall toward the lower end of the Impact Index, with progressively fewer occupying the mid- and high-impact tiers. This pattern reflects the well-known concentration of scholarly influence among a limited subset of journals and demonstrates that the tiering system captures meaningful structure in the underlying citation data.

```{r tierplot, fig.cap= "Journals are grouped into three data-driven tiers (A: Excellent, B: Preferred, C: Acceptable) based on k-means clustering of the Impact Index, a 0–100 score derived from the first principal component of CiteScore, SNIP, and SJR. The distribution shows that most journals fall into the lower-impact tier, while progressively fewer occupy the mid- and high-impact ranges. This reflects the natural stratification of scholarly influence across the behavioral and social sciences."}
tier_plot
```

# Conclusion 
By combining a PCA-derived Impact Index with within-subcategory k-means clustering, this analysis produces a tiering system that reflects real structure in the bibliometric data rather than subjective impressions or arbitrary thresholds. Evaluating journals relative to their disciplinary peers ensures that differences in citation density and publication norms across fields do not distort assessments of journal quality.

The resulting framework is reproducible, principled, and transparent. It provides a flexible tool for guiding publication strategy, mentoring, and evaluation within the behavioral sciences, while remaining adaptable over time. As Scopus metrics evolve and new journals emerge, the system can be readily updated using the same workflow, ensuring continued relevance and methodological consistency.

# Appendix

## Interactive Journal Table
```{r plot}
DT::datatable(
df_final %>%
select(
subcategory,
`Source title`,
tier,
impact_index,
percentile_in_subcategory,
Publisher
) %>%
arrange(subcategory, desc(impact_index)),
extensions = c("Buttons"),
options = list(
pageLength = 25,
autoWidth = TRUE,
dom = "Bfrtip",
buttons = c("copy", "csv", "excel"),
order = list(list(0, "asc"), list(3, "desc"))
),
rownames = FALSE
)
```


